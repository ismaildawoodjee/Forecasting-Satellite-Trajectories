---
output:
  pdf_document: default
  html_document: default
---

\pagenumbering{gobble}

![](C:\Users\Ismail Dawoodjee\Desktop\IDAO 2020\images\cover1.png)

![](C:\Users\Ismail Dawoodjee\Desktop\IDAO 2020\images\cover2.png)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#tinytex::install_tinytex()
# install.packages("bookdown")
```

\newpage
\tableofcontents
\pagenumbering{arabic}

\newpage
# 1. Methodology

The two datasets (in CSV format) used in this study were provided to participants of the 2020 International Data Analytics Olympiad by the Russian Astronomical Science Centre. The first dataset is the Train dataset, which contains 649913 observations and 15 variables. The second is the Test dataset, which contains 284072 observations and 9 variables. The Train dataset consists of real and simulated coordinates (measured in $km$) and velocities (measured in $km/s$) of 600 satellites in the month of January while the Test dataset consists of only simulated coordinates and velocities of 300 satellites in February 2014. The variable names and descriptions for both datasets are summarized in Table 1 below. Moreover, the first five rows of both Train and Test datasets are displayed in Figure 1.

![](C:\Users\Ismail Dawoodjee\Desktop\IDAO 2020\images\table1.png)

![The first 5 rows of Train and Test datasets. It should be noted that only 300 out of 600 satellites from Train are present in Test. For instance, Test does not contain Satellite 0.](C:\Users\Ismail Dawoodjee\Desktop\IDAO 2020\images\figure2.png)

  The $epoch$ variable, as used in the astronomical literature, refers to the moment in time that the elements of a space object, such as its coordinates and velocities, are specified. This should not be confused with the number of epochs of training done for an ML model. The set of *simulated* coordinates and velocities were obtained using the less accurate SGP4 physics-based simulation model while the set of *real* coordinates and velocities were obtained using a more accurate (but unspecified) simulation model. To illustrate this information using an example satellite, the $x$- and $x_{sim}$-coordinates of Satellite 1 from both Train and Test datasets are plotted in Figure 2 below.

![Plots of true and simulated x-coordinates for Satellite 1](C:\Users\Ismail Dawoodjee\Desktop\IDAO 2020\images\figure1.png)

  The green plot displays the real $x$-coordinates of Satellite 1 in January 2014, obtained from the Train dataset. The blue and orange plots are the simulated $x_{sim}$-coordinates in January 2014 (obtained from the Train dataset) and February 2014 (obtained from the Test dataset), respectively.

  Several findings can be deduced by inspection from Figure 2 and from plots of other kinematic states as well (the set of ${x, y, z, V_x, V_y, V_z}$ can be collectively called the *kinematic states*). Firstly, satellite kinematics display a "seasonal" pattern, which makes sense from a physical point of view, because a full orbit around the Earth occurs over a fixed period of time. Secondly, there are no cyclical patterns displayed by the data. To distinguish between cyclical and seasonal patterns, an extract from the book *Forecasting: Principles and Practice* says "If the fluctuations are not of a fixed frequency then they are cyclic; if the frequency is unchanging and associated with some aspect of the calendar, then the pattern is seasonal" (Hyndman and Athanasopoulos, 2018, p.31). Since the fluctuations occur over a fixed orbital period, the pattern should be seasonal. Finally, the simulated coordinates $x_{sim}$ were seen to be initially accurate in predicting true/real coordinates but becomes increasingly inaccurate at future epochs. These findings suggest applying classical forecasting techniques, namely Seasonal Holt-Winters and Seasonal ARIMA methods. However, the focus of this study is on developing machine learning models, so applying classical forecasting methods can be done as an extension to this paper. 
  
  Based on the plots and the above information, three approaches can be taken:
  
1. Ignore the simulated states (coordinates and velocities) and directly predict real states for February using training data of real states and epochs in January. This is equivalent to simply forecasting/extrapolating the green curve into the future. 

$$\text{Predictor variables } \{epoch, x\} \rightarrow \text{ ML model } \rightarrow \text{ Output } \{\hat{x}\}$$

2. Use both real and simulated states in January as training variables to fit the ML model, which can then predict the real states for February.

$$\text{Predictor variables } \{epoch, x, x_{sim}\} \rightarrow \text{ ML model } \rightarrow \text{ Output } \{\hat{x}\}$$
  
3. Prepare the data so that the true errors (e.g. $e_x = x_{sim} - x$) plus the simulated states in January become predictor variables for the ML model, which can then produce the output ML-modified errors (e.g. $\hat{e}_x$) for February. From the ML-modified errors, the real states in February can be retrieved via $\hat{x} = x_{sim} - \hat{e}_x$. This is similar to the approach used by Peng and Bai (2019). 

$$\text{Predictor variables } \{epoch, e_x, x_{sim}\} \rightarrow \text{ ML model } \rightarrow \text{ Output } \{\hat{e}_x\} \rightarrow \{ \hat{x} \}$$

  When combined with the three ML models considered, there are a total of 9 unique ways of tackling the problem of predicting satellite trajectories. Once again, not all combinations will be explored in this paper, investigating only 3 combinations and leaving 6 for further research. In particular, Approach 1 will be used for fitting a Non-Linear Regression model, while Approach 3 will be used for Support Vector Regression and Random Forest Regression models. Furthermore, only the $x$-coordinate will be modelled since the ML models can be readily extended to include the other kinematics states. 

# 2. Metrics and Models

The three metrics that were chosen to evaluate the ML models are RMSE (Root Mean Square Error), SMAPE (Symmetric Mean Absolute Percentage Error) and MAPRE (Mean Absolute Percentage Residual Error). The RMSE is a commonly used metric in regression models, defined by the equation below:

$$RMSE = \sqrt{ \frac{1}{n} \sum_{i = 1}^{n} (\hat{x}_i - x_i)^2 },$$

where $\hat{x}_i$ is the predicted value, $x_i$ is the actual value and $n$ is the number of observations in the dataset. 

The SMAPE metric is less well-known, but it is specialized for use in forecasting applications. It is defined by the equation below:

$$SMAPE = \frac{100\%}{n} \sum_{i = 1}^{n} \frac{|\hat{x}_i - x_i|}{|\hat{x}_i| + |x_i|},$$

where the terms in the equation are the same as those in RMSE.

The MAPRE is a metric introduced by Peng and Bai (2018), in which they used the concept of MAPE (Mean Absolute Percentage Error) to define the MAPRE as follows:

$$MAPRE = 100\% \frac{ \sum_{i = 1}^{n} |e_x - \hat{e}_x| }{ \sum_{i = 1}^{n} |e_x| },$$

where $e_x$ is the true error in the $x$-coordinate and $\hat{e}_x$ is the ML-predicted error. The authors claimed that the learning performance of the ML model is directly quantified by this metric.

The three machine learning models that will be studied in this paper are Non-Linear Regression, Support Vector Regression and Random Forest Regression. 

**Non-Linear Regression** - When faced with a periodic pattern in the data, an intuitive mathematical function for fitting it would be the Fourier series. It is known from mathematics that any periodic pattern that is continuous (need not be differential) can be approximated by a Fourier series. However, based on the periodic pattern observed from Figure 2 in the previous section, a simple sinusoidal curve would be good enough to model the data. Also, due to the slight inclination seen from the figure, a combination of the sine function together with an added linear trend was considered for the regression model. Modelling the $x$-coordinate then turns out as follows:

$$x(t) = A \sin \bigg( \frac{2 \pi}{T} t + \phi \bigg) + mt + c,$$

where $A$ is the amplitude, $T$ is the period, $\phi$ is the phase shift, $m$ and $c$ are the slope and intercept of the linear trend, $t$ is the independent time variable and $x(t)$ is the time-dependent $x$-coordinate. For this model, the built-in `nls` or non-linear least squares function in R will be used to fit the above equation.

**Support Vector Regression (SVR)** - Support vector machines are a robust class of machine learning models that have been preferred by researchers for many years until the advent of artificial neural networks. Part of what makes them robust is their random sampling during cross-validation of the dataset, making the model more generalizable and applicable to new observations. SVMs are usually used for classification but they can also be extended for regression problems, such as the one considered in this study. For regression, the parameters of the SVR are the `kernel`s (can be radial/Gaussian, linear, polynomial, or sigmoid), the maximum margin of error `epsilon` (the higher the epsilon, the higher the model's tolerance to errors), and the `cost` or regularization parameter `C` (the higher the C, the fewer the amount of margin violations above `epsilon`). These parameters should be tuned to obtain the most optimal model possible. The standard `svm` function from the machine learning library `e1071` will be used to implement the SVR model.

**Random Forest Regression (RFR)** - Similar to SVMs, random forests and decision trees are used mainly for classification tasks but can be extended to include regression as well. Random forests use bootstrap aggregation of many decision trees trained on randomly sampled data to obtain a model with lower variance (compared to one decision tree trained on the entire dataset). The parameters that will be tuned for the RFR are: the number of trees to be grown (`ntree`), the number of variables to split at each branch of a tree (`mtry`), and the maximum number of terminal nodes (or leaves) a tree can have (`maxnodes`). The `randomForest` function from the `randomForest` library will be used to implement the RFR model.

# 3. Choosing a Satellite to Study

Out of the 300 satellites in the Train dataset, one satellite was chosen for implementing the 3 models outlined in the previous section. This satellite was chosen on the basis of having the largest number of observations, so as to provide the most training data for the models.

Read in the original datasets. This may take some time because the two files are very large. There are no external libraries required for this section.
```{r}
train <- read.csv("train.csv")
test  <- read.csv("test.csv")
```

Check the dimensions of the datasets.
```{r}
dim(train)
dim(test)
```
The Train dataset has 649912 observations and 15 variables while the Test dataset has 284071 observations and 9 variables. 

Check the variable names of the datasets.
```{r}
names(train)
names(test)
```
The six excluded variables are the true coordinates and velocities, not present in the Test dataset. 

Check the structure of the datasets.
```{r}
str(train)
str(test)
```
Although, the epoch column should be in datetime format, it is given as a Factor object. This issue will be addressed in the data preparation stage.

Note how many observations there are for each satellite.
```{r}
obs <- table(train$sat_id)
head(obs, n = 5)
```

Re-order the number of observations in decreasing order, and subtract 1 because R starts counting at 1 (but sat_id starts counting from 0).
```{r}
sats <- order(obs, decreasing = TRUE) - 1
head(sats, n = 5)
```

Next, obtain the five satellites with the highest number of observations.
```{r}
five_highest <- obs[head(sats, n = 5) + 1]
five_highest
```

Since Satellite 372 has the highest number of observations at 6320, this satellite was chosen.

```{r}
sat_372 <- train[train$sat_id == 372, ]
```

Reset its index.
```{r}
rownames(sat_372) = NULL
```

Finally, the data for Satellite 372 was written into the home directory as a stand-alone dataset.
```{r}
write.csv(x = sat_372, file = "sat_372.csv", row.names = FALSE)
```

# 4. Data Preparation

The libraries used for data preparation were first installed and loaded before preparation on the Satellite 372 dataset.

The "lubridate" library is for handling datetimes while the others are standard libraries for handling functions, models and mappings. To list, the libraries required for this section are `lubridate`, `purrr`, `caret` and `dplyr`.

```{r, message=FALSE}
# install.packages("lubridate")
library(lubridate)

# install.packages("purrr")
library(purrr)

# install.packages("caret")
library(caret)

# install.packages("dplyr")
library(dplyr)
```

Read in the Satellite 372 data.

```{r}
df <- read.csv("sat_372.csv")

dim(df)

sum(is.na(df))

str(df)
```

Satellite 372 data has 6320 observations and 15 variables, with no evidence of missing values. Checking the structure indicates that the epoch column, which should consist of timestamps, is read as a Factor instead. Thus, this variable is converted into the POSIXct datetime format:

```{r}
# set option to convert fractional seconds, up to 3 decimal places
op <- options(digits.secs = 3)

# convert epoch into datetime
df["epoch"] <- map(df["epoch"], ymd_hms)

class(df$epoch)
```

Now that epoch is in proper datetime format, the time difference between two epochs can be calculated. This is important to see because it shows how often the coordinate and velocity data were sampled from the simulations.

```{r}
num_obs <- dim(df)[1]

# calculate time difference between two successive epochs
time_diff <- df[2:num_obs, "epoch"] - df[1:num_obs-1, "epoch"]

length(time_diff) # 6319 observations, as opposed to 6320 in the dataset

# the first element shouldn't be there because there is no epoch 0 before epoch 1
# so fill that entry with the next element
time_diff <- prepend(time_diff, time_diff[1])

df["delta_t"] <- time_diff
```
Next, the absolute record of time, starting from January 1 00:00:00 as being the zero point in time, can be calculated and added as another column. Absolute time will be a useful additional predictor variable for training machine learning models.

```{r}
df["abs_time"] <- df$epoch - df[1,"epoch"]

head(df$abs_time)
```

In order to follow the approach used by Peng and Bai (2019), which is Approach 3 described in the Methodology section, six error variables were added for each of the six kinematic states.

```{r}
df["e_x"]  = df["x"]  - df["x_sim"]
df["e_y"]  = df["y"]  - df["y_sim"]  
df["e_z"]  = df["z"]  - df["z_sim"]
df["e_Vx"] = df["Vx"] - df["Vx_sim"]
df["e_Vy"] = df["Vy"] - df["Vy_sim"]
df["e_Vz"] = df["Vz"] - df["Vz_sim"]
```

When using Approach 3, these error columns will be the target variables.

While exploring the `delta_t` variable, an anomaly was detected. For the purpose of data preparation, the steps in removing this anomaly was added to this section although the same steps were done in the subsequent Data Exploration section.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
"
# delete anomaly at ID 750171 (please refer to Section 5a: Data Exploration 1)
df <- filter(df, df$id != 750171)
anomaly_index <- as.numeric(rownames(df[df$id == 750172,]))
df[anomaly_index, 'delta_t'] <- df[anomaly_index-1, 'delta_t']
"
```

Finally, the dataset can be split into Train and Test components. The Test data was taken from the last 7 days while the Train dataset was taken from the first 25 days. This is because 7 days is the minimum time period in which response to imminent threats, such as collisions between space objects, can be initiated to avoid that danger (Peng and Bai, 2019).

```{r}
# split data into Train and Test. Test is last 7 days
# Train is from days 1:24 and Test is from days 25:31
train <- filter(df, as.numeric(unlist(map(df['epoch'], day))) < 25)
test  <- filter(df, as.numeric(unlist(map(df['epoch'], day))) >= 25)
```

Export Prepared, Train and Test datasets to the home directory.
```{r eval=FALSE}
write.csv(x = df, file = "sat_372_prepared.csv", row.names = FALSE)
write.csv(x = train, file = "sat_372_train.csv", row.names = FALSE)
write.csv(x = test, file = "sat_372_test.csv", row.names = FALSE)
```

# 5a. Data Exploration Part 1

Load the libraries used for data exploration, read in the dataset and change `epoch` class to datetime. The libraries required for this section are `lubridate` and `ggplot2`.

```{r message = FALSE}
library(lubridate)
library(ggplot2)
```

```{r}
df <- read.csv("sat_372_prepared.csv")

# change epoch class to POSIXct
df$epoch <- as.POSIXct(df$epoch, tz = "UTC")

# set ggplot black and white background theme
theme_set(theme_bw())
```

Firstly, all 6320 observations for the variable `x` were plotted over 31 days to see how it varies.

```{r}
ggplot(
  aes(x = epoch),
  data = df
) + geom_line(aes(y = x), color = "steelblue") +
    labs(title = "True Coordinates x")
```


The plot above indicates a somewhat periodic pattern, but it is difficult to see the detailed features of the graph, so only the first 204 observations will plotted in subsequent visualizations. There are 6320 observations over 31 days, so one day is approximately 204 observations, assuming equal time interval between observations. Moreover, to see the comparison between the true and simulated coordinates, both of them were plotted on the same figure.

```{r}
ggplot(
  aes(x = epoch),
  data = head(df, 204)
) + geom_line(aes(y = x_sim, colour = "Simulated")) +
    geom_point(aes(y = x,     colour = "True")) +
    scale_colour_manual(values = c("red", "steelblue")) +
    labs(title = "January Day 1 x-coordinates", colour = "Coordinates") +
    ylab("position (km)")
```

At first glance, it looks as if the simulated values are very good at approximating the true coordinate. To see if this accuracy persists, the same plot on Day 22 was done.

```{r}
start = 204 * 21
stop = 204 * 22

ggplot(
  aes(x = epoch),
  data = df[start:stop,]
) + geom_line(aes(y = x_sim, colour = "Simulated")) +
    geom_point(aes(y = x,     colour = "True")) +
    scale_colour_manual(values = c("red", "steelblue")) +
    labs(title = "January Day 22 x-coordinates", colour = "Coordinates") +
    ylab("position (km)")
```

A clear lag (or phase shift) between the true and simulated coordinates can be observed. Moreover, the overall plot for the entire 31 days was plotted.

```{r}
ggplot(
  aes(x = epoch),
  data = df
) + geom_line(aes(y = x_sim, colour = "Simulated")) +
    geom_line(aes(y = x,     colour = "True")) +
    scale_colour_manual(values = c("red", "steelblue")) +
    labs(title = "January x-coordinates", colour = "Coordinates") +
    ylab("position (km)")
```

Again, it is somewhat diffucult to make out the details, but the fitting capacity of the simulated values on the true coordinates becomes poorer as time passes. This brings the discussion to visualizing the errors themselves, for instance, the error in $x$ given by the variable $e_x = x - x_{sim}$.

```{r}
ggplot(
  aes(x = epoch),
  data = df
) + geom_line(aes(y = e_x), color = "steelblue") +
    labs(title = "Error in x (km)")
```

The $e_x$ variable looks like a spade-shaped object - in which the error starts out small and increases (this was expected) but peaks around Day 21-23 and then decreases to a small value at Day 30 (this was unexpected). After this, the error increases over Day 31. Visualizing the other errors also indicates the same spade-shaped pattern, albeit with the peaks occurring on different days.

Thus, it can be presumed that the simulated coordinates lag behind the true coordinates by some phase shift, which then corrects itself over 30 days. In fact, the plot above indicates two kinds of errors: the error in predicting the true coordinate, and the failure to do so at the correct epochs.

Similar visualizations can be done for the remaining five kinematic states.

Next, the anomaly mentioned in Section 2: Data Preparation needs to be dealt with. This anomaly was first discovered when exploring the `delta_t` variable.

```{r}
table(df$delta_t)
```

One observation has a `delta_t` value significantly different from the others (approximately 0.001s). Look at that observation:

```{r}
filter(df, df$delta_t < 1)
```

This anomaly is at ID 750172. Looking at the two observations on either side of it could provide more information.

```{r}
filter(df, df$id %in% c(750170:750174))
```

From inspecting the `epoch` and `abs_time` variables, it seems as though the simulation was updated within a millisecond, because while $x_{sim}$ changes by a lot (from -1765.593 to 804.142), $x$ changes very little (from 8581.835 to 8581.831) within that millisecond. 

The solution to this anomaly was to keep the updated observation (which is ID 750172) and delete the old observation (ID 750171).

```{r}
df <- filter(df, df$id != 750171)

# replace the 1 millisecond value with the delta_t value before it
anomaly_index <- as.numeric(rownames(df[df$id == 750172,]))
df[anomaly_index, 'delta_t'] <- df[anomaly_index-1, 'delta_t']

```

Other than this anomaly, coordinate data was seen to be sampled at equally spaced intervls of 7.06 minutes.

```{r}
mean(df$delta_t) / 60
sd(df$delta_t) / 60
```

Finally, it is suggested for the reader to rerun the code chunk from Section 4: Data Preparation to take into account this anomaly before splitting the dataset into Train and Test. 

# 5b. Data Exploration Part 2

This section explores the correlation between variables as well as testing for normality and deducing the orbit type of Satellite 372. The libraries required for this section are `Hmisc` and `corrplot`.

```{r message=FALSE}
#install.packages("Hmisc")
library("Hmisc")

#install.packages("corrplot")
library("corrplot")

df <- read.csv("sat_372_prepared.csv")

df$epoch <- as.POSIXct(df$epoch, tz = "UTC")
```

Confirm that the anomaly detected from Section 3: Data Exploration is absent.
```{r}
dim(df)
```
The dataset should have 6319 observations. Otherwise, the anomaly is still present. 

Firstly, a correlation matrix between all the relevant variables was calculated and rounded to three significant figures. After this, a heatmap (including the dendrogram) and correlation plot was graphed to visualize it.

```{r}
corrmat <- round(rcorr(as.matrix(df[c(4:23)]))$r, 3)
#corrmat

# visualize a heatmap of the correlation matrix
heatmap(x = corrmat, symm = TRUE)
```

Another visualization of the correlation matrix:
```{r}
corrplot(corrmat, type = "upper", tl.col = "black",
         sig.level = 0.01, insig = "blank")
```

The above plot indicates a stronger correlation between simulated states and errors, as opposed to the weaker correlation between simulated states and true states. This justifies the use of Approach 3 over Approach 2 (discussed in Section 2: Methodology) in treating the errors as the target variables instead of true states. 

Next, the normality of each variable was tested using the Shapiro-Wilk's test for the first 5000 observations.

```{r}
# Shapiro-Wilk's test for the first 5000 observations
for (i in c(4:23)) {
  print(names(df)[i])
  print(shapiro.test(x = df[1:5000, i]))
}
```

All the variables indicate a significant p-value, which means they are all non-normal. Thus, for the purpose of scaling data, min-max normalization should be used instead of standardization. 

Finally, the orbit type of Satellite 372 can be determined by calculating its radius of orbit.

```{r}
radius <- sqrt( (df$x)^2 + (df$y)^2 + (df$z)^2 )

summary(radius)
```
The satellite orbit radius ranges from a minimum of 8166 km to a maximum of 12027 km, with a mean radius of 10248 km. This orbit is greater than the 2000 km threshold for Low Earth Orbit but less than the 35786 km lower limit for High Earth Orbit. Hence, Satellite 372 is an MEO (Medium Earth Orbit) object.  

# 6. Data Preprocessing

Following the Shapiro-Wilk's normality test done in the previous section, standardization will not be done. Instead, min-max normalization will be applied to the predictor variables, which are the simulated states. Moreover, the target variables should not be scaled.

The library required for this section is `caret`.
```{r}
library(caret)

train <- read.csv("sat_372_train.csv")
test  <- read.csv("sat_372_test.csv")
```

To prevent data leakage, which means making use of data from sources other than the training data, the min-max normalization was only fitted to the Train dataset. 
```{r}
# the fitted normalization parameters from the Train dataset
prep_params <- preProcess(train[c(10:15,17)], method = c("range"))
prep_params
```

Using these parameters, both Train and Test data was normalized and updated using min-max scaling. 

```{r}
train_norm <- predict(prep_params, train)
test_norm  <- predict(prep_params, test)
```

To check if the variables are properly normalized, call on the `summary` function:
```{r}
summary(train_norm)
summary(test_norm)
```

All simulated states in the Train dataset are now within the interval $[0,1]$. For the Test dataset, some simulated states contain negative values, which is the result of fitting the normalization parameters on the Train dataset only. 

The normalized data was then exported to the Home Directory.

```{r}
write.csv(x = train_norm, file = "sat_372_normalized_train.csv", row.names = FALSE)
write.csv(x = test_norm,  file = "sat_372_normalized_test.csv",  row.names = FALSE)
```

# 7. Non-Linear Regression Model

When implementing a non-linear fit, there is no need to use normalized data. Therefore, the non-normalized prepared datasets, including the splitted Train and Test, were loaded. The libraries required for this section are `caret` and `Metrics`.

```{r}
library(caret)

#install.packages("Metrics")
library(Metrics)

train <- read.csv("sat_372_train.csv")
test  <- read.csv("sat_372_test.csv")
```

One problem arose while implementing the non-linear regression. Fitting on all obervations in the Train dataset results in non-sensical values for the model parameters. After trying out various amounts of observations, the last 979 observations of the Train dataset were used to fit the model because it gave one of the lowest residual standard errors. Moreover, it resulted in statistically significant values for the model parameters. 

```{r}
# train model on the last 1/5th of the Train data
len <- ceiling(dim(train)[1] / 5)
df  <- tail(train, len)

time <- df[, "abs_time"]
x    <- df[, "x"]
sine_func <- x ~ A*sin(2*pi*time/t + phi) + m*time + c
```

Fitting the `nls` function requires the user to provide starting values. Some of these values were obtained by inspecting a plot of the final day (Day 31) in the Train dataset and approximating them by eye. 

```{r}
plot(x = tail(time, 204), y = tail(x, 204), type = "l")
```

From the above plot, the x-coordinate varies between -7000 to 12000, so the amplitude `A` was estimated as 10000. Likewise, the time difference between every other peak (or trough) appears to be separated by 20000 seconds, so the period `t` of the curve was estimated to be 10000. The phase shift `phi` and intercept `c` was just provided as 1 while the very gentle upward slope `m` was given the small positive starting value 0.01. These parameters need not be accurate as the model will automatically find the best fitted parameters.

After inspection, the non-linear regression model was implemented.

```{r}
nls_model <- nls(formula = sine_func,
                 data = data.frame(time, x),
                 start = list(A = 10000, t = 10000, phi = 1, m = 0.01, c = 1))
```

The model details, summary and coefficients can be viewed below.

```{r}
nls_model

summary(nls_model)

coefficients(nls_model)

param <- as.numeric(coefficients(nls_model))
param
```

These parameters define an upward sloping sine curve, which can be extrapolated over the epochs from the Test dataset.

```{r}
predict_state <- function(state, parameters, data) {
  
  # `state` is the desired kinematic state; must be char(x,y,z,Vx,Vy,Vz)
  # `parameters` are the model parameters obtained from fitting nls
  # `data` can be the Test data, but can also be observations from Train
  
  t  <- data[, "abs_time"]
  k  <- data[, state]
  k_hat <- param[1] * sin(2*pi*t / param[2] + param[3]) + param[4]*t + param[5] 
  
  return(k_hat)
    
}

x_hat_nls <- predict_state(state = "x", parameters = param, data = test)
```

The performance of the model was tested on three metrics: RMSE, SMAPE and MAPRE. RMSE and SMAPE are already part of the `Metrics` library in R but the MAPRE metric needs to be manually defined. 

```{r}
mapre <- function(actual, predicted) {
  
  metric <- 100 * (sum(abs(actual - predicted))) / (sum(abs(actual)))
  
  return(metric)
  
}
```

Note that the following usage of the MAPRE metric does not conform with the definition presented in Peng and Bai (2019) because it uses coordinates instead of errors. Moreover, R's definition of the SMAPE metric confines the output within the interval $[0,2]$, so the resulting metric needs to be divided by 2 and multiplied by 100% to give the standard percentage value.

```{r}
evaluate_model <- function(true, estimated) {

  r <- rmse (actual = true, predicted = estimated)
  s <- smape(actual = true, predicted = estimated) * 100/2
  m <- mapre(actual = true, predicted = estimated)

  eval = c(r,s,m)
  names(eval) <- c("RMSE", "SMAPE", "MAPRE")
  
  return(eval)
  
}

nls_eval = evaluate_model(true = test$x, estimated = x_hat_nls)
nls_eval
```

Similar models can be built for the remaining kinematic states, but will not be done in this study. 

# 8. Support Vector Regression Model

For implementing the SVR model, normalized data needs to be used. The libraries required for this section are `ggplot2`, `Metrics` and `e1071`. Additionally, the user-created `mapre` and `evaluate_model` functions from the previous section need to be activated. 

```{r}
library(ggplot2)
library(e1071)
library(Metrics)

train <- read.csv("sat_372_normalized_train.csv")
test  <- read.csv("sat_372_normalized_test.csv")

func <- e_x ~ abs_time + x_sim + y_sim + z_sim + Vx_sim + Vy_sim + Vz_sim
```

The dependent variable to be predicted is the error in `x` given by `e_x`. The predictor variables are the simulated kinematic states. Initially, the default SVR model without tuning was implemented and the error metrics calculated.

```{r}
svr_default <- svm(formula = func, data = train, scale = FALSE)
summary(svr_default)
e_x_hat_svr1 <- predict(svr_default, test)
svr_default_eval <- evaluate_model(true = test$e_x, estimated = e_x_hat_svr1)
svr_default_eval
```
The results are not very promising compared to the non-linear regression model. However, more optimal values of the hyperparameters `epsilon` and `cost` were found through a grid search. 

```{r}
"
#Tuning the hyperparameters epsilon and C

set.seed(123)
svr_default_tuned <- tune(method = svm, train.x = func, data = train,
                    ranges = list(epsilon = seq(0,1,0.1), cost = 2^(0:9)))
plot(svr_default_tuned)
summary(svr_default_tuned)

opt_model = svr_default_tuned$best.model
summary(opt_model)

e_x_hat_svr1 <- predict(opt_model, test)
svr_default_eval <- evaluate_model(true = test$e_x, estimated = e_x_hat_svr1)
svr_default_eval #cost=512, gamma=0.25, epsilon=0
"

svr_default_tuned <- svm(formula = func, data = train, scale = FALSE, gamma = 0.25, epsilon = 0, cost = 512)
summary(svr_default_tuned)
e_x_hat_svr1_tuned <- predict(svr_default_tuned, test)
svr_default_tuned_eval <- evaluate_model(true = test$e_x, estimated = e_x_hat_svr1_tuned)
svr_default_tuned_eval
```

The grid search takes a long time to run because it searches over a space of 110 (11 by 10) possible combinations of epsilon and C. To improve computational time, a localized approach to the grid search can be taken to reduce the time required. Also, the evaluation is not much of an improvement either. The SMAPE score decreases, which indicates better forecasting accuracy, but the RMSE and MAPRE increased more than twice as compared to the untuned model. This was thought to be due to grid search's inherent random sampling when carrying out 10-fold cross-validation, which is unsuitable for time series or time-dependent data in chronological order.

The next SVR model will use the linear kernel instead of the default radial one.


```{r}
svr_linear <- svm(formula = func, data = train, scale = FALSE, kernel = "linear")
summary(svr_linear)
e_x_hat_svr2 <- predict(svr_linear, test)
svr_linear_eval <- evaluate_model(true = test$e_x, estimated = e_x_hat_svr2)
svr_linear_eval

```

These results are a slight improvement over the radial kernel model. This time, the model tuning will also consider the kernel hyperparameter in addition to `epsilon` and `cost`.

```{r}
"
#Tuning the hyperparameters epsilon and C

set.seed(123)
svr_tuned <- tune(method = svm, train.x = func, data = train,
                  ranges = list(epsilon = seq(0,1,0.5), 
                                cost = 2^(seq(0,10,5)),
                                kernel = c('radial', 'linear', 'sigmoid')))
#plot(svr_tuned)
summary(svr_tuned)

opt_model = svr_tuned$best.model
summary(opt_model)

e_x_hat_svr1 <- predict(opt_model, test)
svr_tuned_eval <- evaluate_model(true = test$e_x, estimated = e_x_hat_svr1)
svr_default_eval #cost=512, gamma=0.25, epsilon=0

# Retune more finely
svr_retuned <- tune(method = svm, train.x = func, data = train, scale = FALSE,
                    ranges = list(epsilon = seq(0,0.5,0.25), 
                                cost = 2^(seq(5,10,2.5)),
                                kernel = 'radial'))
summary(svr_retuned)

opt_model = svr_retuned$best.model
summary(opt_model)

e_x_hat_svr1 <- predict(opt_model, test)
svr_retuned_eval <- evaluate_model(true = test$e_x, estimated = e_x_hat_svr1)
svr_retuned_eval
"
```

RMSE and MAPRE improved but this came at the cost of a higher SMAPE. Finally, a sigmoid kernel SVR model was fitted without tuning.


```{r}
svr_sigmoid <- svm(formula = func, data = train, scale = FALSE, kernel = "sigmoid")
summary(svr_sigmoid)
e_x_hat_svr3 <- predict(svr_sigmoid, test)
svr_sigmoid_eval <- evaluate_model(true = test$e_x, estimated = e_x_hat_svr3)
svr_sigmoid_eval
```

The untuned sigmoid SVR model performed slightly worse than the untuned linear SVR model across all metrics. Hyperparameter tuning improved the SMAPE metric but worsened the other metrics. Overall, the SVR models, even after tuning the hyperparameters, did not perform as well as the non-linear fit, most probably due to random sampling during cross-validation. 

# 9. Random Forest Regression

The third and final model is the Random Forest Regression (RFR) model. Usually, decision trees and random forests are used in classification tasks, but they can be extended to regression problems as well. The libraries required for this section are `randomForest` and `caret`. Moreover, the user-created functions `mapre` and `evaluate_model` in Section: Non-Linear Regression need to be activated. Since scaling is not necessary for random forests, the unscaled datasets were used.

```{r}
# install.packages("randomForest")
library(randomForest)
library(caret)
library(Metrics)

train <- read.csv("sat_372_train.csv")
test  <- read.csv("sat_372_test.csv")
```

The random forest model requires a formula (given below) `formula`, the number of trees `ntree`, and the number of variables that are randomly selected to be partitioned at each split in a tree `mtry`. The default values of `ntree = 500` trees and (7 variables/3) `mtry = 2` variables at each split were used initially.

```{r}
func <- e_x ~ abs_time + x_sim + y_sim + z_sim + Vx_sim + Vy_sim + Vz_sim

set.seed(123)
rf_default <- randomForest(formula = func, data = train)
print(rf_default)
attributes(rf_default)
varImpPlot(rf_default)
plot(rf_default)
```

An noteworthy observation from the Variable Importance Plot is that the variable `abs_time` was seen to be most important out of the 7 variables considered, with `Vz_sim` and `x_sim` having similar importance. Moreover, the model performance levels off after around 100 to 200 trees. After fitting the model, predictions were made.

```{r}
e_x_hat_rf1 <- predict(rf_default, test)
rf_default_eval <- evaluate_model(true = test$e_x, estimated = e_x_hat_rf1)
rf_default_eval
```

The results display a similar pattern to the tuned SVR models, with a lower SMAPE score but higher RMSE and MAPRE. 

Now the most optimal parameters of the regression model will be found through grid search tuning. Tuning the random forest involves tuning the number of trees `ntree`, the number of variable splits `mtry`, and the maximum number of nodes `maxnodes`.

```{r}
"
# This code finds the best value of `mtry`.
set.seed(123)
tuneGrid <- expand.grid(.mtry = c(1: 7))
trControl <- trainControl(method = 'timeslice',
                          number = 5,
                          search = 'grid',
                          initialWindow = 1000,
                          horizon = 200,
                          fixedWindow = TRUE)
rf_mtry <- train(func,
    data = train,
    method = 'rf',
    metric = 'RMSE',
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    nodesize = 10,
    ntree = 250)
print(rf_mtry) # 7 is the best
best_mtry <- rf_mtry$bestTune$mtry
best_mtry
"
```

The best value of `mtry` was found to be 7. The next grid search looks at the best value of `maxnodes`.

```{r}
"
# This code finds the best value of `maxnodes`.
set.seed(123)
tuneGrid <- expand.grid(.mtry = 7)
trControl <- trainControl(method = 'timeslice',
                          number = 5,
                          search = 'grid',
                          initialWindow = 1200,
                          horizon = 1000,
                          fixedWindow = TRUE)
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = 7)
for (maxnodes in c(5: 15)) {
    set.seed(123)
    rf_maxnode <- train(func,
        data = train,
        method = 'rf',
        metric = 'RMSE',
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        nodesize = 10,
        maxnodes = maxnodes,
        ntree = 250)
    current_iteration <- toString(maxnodes)
    store_maxnode[[current_iteration]] <- rf_maxnode
}
results_maxnode <- resamples(store_maxnode)
summary(results_maxnode) # 15 is the best
"
```

Using 15 as the best value for `maxnodes` and 7 for `mtry`, the following code finds the best value for the number of trees `ntree`.

```{r}
"
# This code finds the best value of `ntree`.
set.seed(123)
tuneGrid <- expand.grid(.mtry = 7)
trControl <- trainControl(method = 'timeslice',
                          number = 5,
                          search = 'grid',
                          initialWindow = 1200,
                          horizon = 1000,
                          fixedWindow = TRUE)

store_maxtrees <- list()
for (ntree in c(50, 100, 150, 200, 250, 300, 350, 400, 500)) {
    set.seed(123)
    rf_maxtrees <- train(func,
        data = train,
        method = 'rf',
        metric = 'RMSE',
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        nodesize = 10,
        maxnodes = 15,
        ntree = ntree)
    key <- toString(ntree)
    store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
summary(results_tree) # choose 200 trees even though 500 is the best
"
```

The results indicate very little improvement in error when using 500 trees over 50 trees. Hence, to be on the safe side, use 200 trees as the best `ntree` value. In summary, `mtry = 7`, `maxnodes = 15` and `ntree = 200` will be used to build the optimal random forest regression model.

```{r}
func <- e_x ~ abs_time + x_sim + y_sim + z_sim + Vx_sim + Vy_sim + Vz_sim

set.seed(123)
rf_optimal <- randomForest(formula = func, data = train, ntree = 200, mtry = 7, nodesize = 15)
print(rf_optimal)
attributes(rf_optimal)
varImpPlot(rf_optimal)
plot(rf_optimal)
```

In the optimal random forest model, the most important variable was found to be `x_sim`, followed by `abs_time`, while the last three variables `Vx_sim`, `y_sim` and `Vy_sim` were seen to be of low (and roughly similar) importance. 

```{r}
e_x_hat_rf_opt <- predict(rf_optimal, test)
rf_optimal_eval <- evaluate_model(true = test$e_x, estimated = e_x_hat_rf_opt)
rf_optimal_eval

```

Evaluating the optimized model results in better performance, since all three metrics are lower than the un-optimized model. 

# 10. Analysis and Recommendations

So far, all the models that were built were trained on the Train dataset and the evaluated results were obtained by testing the models on the Test dataset. This misses out a crucial aspect of machine learning, namely, validation of the models. Validation can be done through comparing the performance metrics across both Train and Test datasets, in order to determine whether overfitting or underfitting occurs. 

Here, a concise tabular summary of the model performances across the three metrics, tested only on Test data, was constructed as shown below:

```{r}
# the evaluation vector of the tuned linear SVR model was not saved, 
# so recreate it here
svr_default_retuned_eval <- c(11310.53790, 69.57217, 187.43788)
names(svr_default_retuned_eval) <- c("RMSE", "SMAPE", "MAPRE")

model_names <- c("Non-Linear Regression", "SVR Radial", "SVR Radial Tuned", 
                 "SVR Radial Retuned", "SVR Linear", "SVR Sigmoid", 
                 "Random Forest", "Random Forest Tuned")

model_performances <- data.frame(nls_eval, svr_default_eval, svr_default_tuned_eval, 
                                 svr_default_retuned_eval, svr_linear_eval,
                                 svr_sigmoid_eval, rf_default_eval, rf_optimal_eval)

colnames(model_performances) <- model_names

# take transpose of the table
model_performances <- t(model_performances)
model_performances
```

Note that these results were obtained only for the `x` coordinate. Similar steps to model building may be done for the other five kinematic states. After putting the Test results in tabular form above, a simple validation of each model can be done by testing the machine learning models on the Train dataset, and then comparing the difference in performance.

```{r}
# the evaluation vectors will have the subscript `v` when tested on Train data

# non-linear regression model on Train data
x_hat_nls_v <- predict_state(state = "x", parameters = param, data = train)
nls_eval_v = evaluate_model(true = train$x, estimated = x_hat_nls_v)

# SVR radial model on Train data
e_x_hat_svr1_v <- predict(svr_default, train)
svr_default_eval_v <- evaluate_model(true = train$e_x, estimated = e_x_hat_svr1_v)

# SVR radial tuned model on Train data
e_x_hat_svr1_tuned_v <- predict(svr_default_tuned, train)
svr_default_tuned_eval_v <- evaluate_model(true = train$e_x, estimated = e_x_hat_svr1_tuned_v)

# Unfortunately, the grid search results of the retuned/fine-tuned SVR radial model was not saved.
# Also, running the grid search took a long time, so the retuned model will be skipped

# SVR linear model on Train data
e_x_hat_svr2_v <- predict(svr_linear, train)
svr_linear_eval_v <- evaluate_model(true = train$e_x, estimated = e_x_hat_svr2_v)

# SVR sigmoid model on Traan data
e_x_hat_svr3_v <- predict(svr_sigmoid, train)
svr_sigmoid_eval_v <- evaluate_model(true = train$e_x, estimated = e_x_hat_svr3_v)

# Random forest model on Train data
e_x_hat_rf1_v <- predict(rf_default, train)
rf_default_eval_v <- evaluate_model(true = train$e_x, estimated = e_x_hat_rf1_v)

# Random forest tuned model on Train data 
e_x_hat_rf_opt_v <- predict(rf_optimal, train)
rf_optimal_eval_v <- evaluate_model(true = train$e_x, estimated = e_x_hat_rf_opt_v)

```

A similar table for the model performances on Train data was then constructed below:

```{r}
model_names_v <- model_names <- c("Non-Linear Regression", "SVR Radial", "SVR Radial Tuned", 
                                  "SVR Linear", "SVR Sigmoid", 
                                  "Random Forest", "Random Forest Tuned")

model_performances_v <- data.frame(nls_eval_v, svr_default_eval_v, 
                                   svr_default_tuned_eval_v, svr_linear_eval_v, 
                                   svr_sigmoid_eval_v, rf_default_eval_v, rf_optimal_eval_v)

colnames(model_performances_v) <- model_names_v

model_performances_v <- t(round(model_performances_v,3))
model_performances_v
```

A comparison of the two tables can be made by calculating the percentage difference in error between them, defined in the code chunk below. If the percentage difference is positive, then the model has been overfitted because the error in the Test dataset is higher than the error in the Train dataset, and vice versa. 

```{r}
test_eval  <- model_performances[-4,]
train_eval <- model_performances_v

# calculate percentage difference in error 
perc_diff <- round((test_eval - train_eval) * 100 / train_eval, 2)
perc_diff

# negative means underfit, positive means overfit

```

Of course, this is a naive way of validation, since it does not take multiple training epochs into account and simply looks at one particular instance of a model at one time. However, it does give a general ballpark indicator of how serious the overfit/underfit is. For instance, the SVR Linear kernel model indicates a severe case of underfitting while the Tuned Random Forest model indicates the opposite. The models that appear to be generalizable across both datasets are the SVR Radial and SVR Sigmoid models. In addition, the tuned models seem to overfit much more than the corresponding un-tuned models. The underfit in the non-linear regression model is due to some quirk of the `nls` function not being able to fit on large amounts of data, but fits very well on about 1000 observations. Finally, the metric that reacts the least to overfitting/underfitting is the SMAPE metric, due to its low percentage difference. This could be due to SMAPE being a specialized metric for time-dependent data.

From this analysis, the best model was chosen to be the non-linear regression model due to its simplicity and interpretability as well as its performance on the Test dataset. Due to the time-dependent nature of the datasets, the best metric to measure performance would be the SMAPE metric. 

# 11. Conclusion

To recap, this study looked at data on satellite kinematics and used three machine learning models to predict the $x$-coordinates of one satellite. This satellite (Satellite 372) was chosen on the basis of having the highest number of observations. Based on the periodic pattern observed in the coordinate data, the first model proposed was a non-linear sine curve regression with an additive linear trend. The second and third models were Support Vector Regression and Random Forest Regression. The Non-Linear Regression model was observed to have the best performance out of the three, most probably due to fitting the correct periodic function. The other two models performed poorly. This was thought to be due to random sampling, which is not appropriate for data arranged in chronological order. 

There are several improvements and extensions that could be made to this study. Firstly, only a small subset of the given dataset was used. Out of 600 satellites, only one was studied. Moreover, out of the six kinematic states, only the $x$-coordinate (and its error) were predicted. Also, the true states for the Test dataset were not provided to competitors, so the evaluation of the proposed models was restricted to the Train dataset. Thus, this study effectively looked at only 1/3600 of the entire dataset. 

Secondly, an in-depth exploration and discussion on the other 5 kinematic states could be done. These states were explored but were not included and discussed within the study. Also, the validation of the proposed models were unsatisfactory at best, so better validation techniques that are different from randomly sampled cross-validation (and especially designed for time-dependent data), could be studied and applied instead.

Thirdly, only three machine learning models were proposed in this study. Gaussian process regression and the multilayer perceptron could be explored as an extension. In addition, classical forecasting techniques such as Seasonal Holt-Winters and Seasonal ARIMA models could be quite ideal in forecasting/predicting satellite coordinates, even if they are not machine learning models. 

# Acknowledgements

The author thanks the Russian Astronomical Science Center (and the hosts, partners and organizers of the International Data Analytics Olympiad) for providing the data sets used in this study.

# References

![](C:\Users\Ismail Dawoodjee\Desktop\IDAO 2020\images\references.png)

